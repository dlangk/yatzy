:::section{#part-vi}

:::part-title
Part VI
:::

## Compressing Genius

The solver produces a 2-million-entry lookup table --a complete map from
every reachable game state to the optimal action. It is perfect and utterly
opaque. No human could memorise it, and no simple rule set can replicate it.
But how small a model can approximate it? This is the question of
::concept[supervised distillation]{supervised-distillation}:
compressing an oracle's knowledge into a learnable function.

The natural first instinct is reinforcement learning --let an agent
play millions of games and learn from reward. But Yatzy presents a severe
::concept[credit assignment]{credit-assignment}
problem. A single game spans 15 rounds with roughly 45 decisions. The final
score is a single number. When the agent scores poorly, which of the 45
decisions was the mistake? The reward is sparse, delayed, and entangled
across a long horizon. Standard RL algorithms (PPO, DQN) struggle to
propagate credit through this many steps without enormous sample budgets,
and even then they plateau well below optimal play.

:::html
<div class="chart-container" id="chart-rl-barrier-diagram"><div id="chart-rl-barrier-diagram-svg"></div></div>
:::

Supervised distillation sidesteps credit assignment entirely. The solver
generates 200,000 games under optimal play, recording every decision point:
the game state, the optimal action, and the value gap between the best and
second-best action. This produces roughly 3 million labelled examples per
decision type (category selection, first reroll, second reroll). A surrogate
model then learns to map states to actions by pure classification, with no
need to discover the reward structure.

The results reveal a clear winner:
::concept[decision trees]{decision-tree}
dominate MLPs at every parameter budget. A depth-20 decision tree ensemble
(413K total parameters across three decision types) achieves a mean game
score of 245 --just 3 points below the optimal 248. A comparably-sized
MLP achieves only 241. The gap widens at smaller scales: at 2,000 parameters,
a depth-10 decision tree scores 216 while a two-layer MLP of similar size
manages 221 with five times the parameters.

:::html
<div class="chart-container" id="chart-surrogate-pareto"><div id="chart-surrogate-pareto-svg"></div></div>
:::

Why do trees win? The optimal policy contains many hard boundaries: "if Yatzy
is available and you have four of a kind, always keep." These axis-aligned
splits are exactly what decision trees learn. MLPs must waste capacity learning
smooth approximations to step functions. The combinatorial, discrete nature of
the action space --15 categories, 32 possible keeps --favours the
tree's ability to carve the feature space into crisp regions.

The compression curve reveals where human-level play sits. A mean score of
220–230 (the range most experienced human players achieve) requires
roughly 6,000 to 15,000 parameters. Below this, the upper bonus rate collapses:
a depth-8 tree (1,878 parameters) secures the bonus in only 35.6% of games
versus 84.4% for the depth-20 tree. The 50-point bonus is the single biggest
differentiator between weak and strong play, and a model must have enough
capacity to track the upper-section score and route decisions accordingly.
This is a concrete expression of
::concept[resource rationality]{resource-rationality}:
human cognitive limits place us at a specific point on the Pareto frontier
between model complexity and playing strength.

:::html
<div class="chart-container" id="chart-compression-long-tail"><div id="chart-compression-long-tail-svg"></div></div>
:::

At the other end, even the best surrogate (a full unconstrained tree at 280K
parameters) still loses 0.9 points per game to the lookup table. The residual
errors cluster near the upper bonus threshold --states where the score
is between 40 and 62 and the bonus decision is razor-thin. These are the
hardest decisions in the game, and they resist compression because they depend
on subtle interactions between remaining categories, current upper score, and
future reroll distributions.

:::depth-2

### Training Data and Feature Representation

The training data is generated by simulating 200K games under &theta; = 0
optimal play. At each decision point, the feature vector encodes:

:::html
<table>
  <thead>
    <tr><th>Feature Group</th><th class="num">Count</th><th>Description</th></tr>
  </thead>
  <tbody>
    <tr><td>Dice face counts</td><td class="num">6</td><td>Count of each face value (1–6)</td></tr>
    <tr><td>Dice aggregates</td><td class="num">4</td><td>Sum, max count, distinct faces, max face</td></tr>
    <tr><td>Category availability</td><td class="num">15</td><td>Binary: is each category still open?</td></tr>
    <tr><td>Game state</td><td class="num">4</td><td>Turn number, upper score, bonus achieved, rerolls left</td></tr>
  </tbody>
</table>
:::

This 29–30 feature representation is verified to be *lossless*:
across 1.7 million unique feature vectors per decision type, zero label
conflicts exist. Every unique game state maps to exactly one optimal action.

### The Pareto Frontier

:::html
<table>
  <thead>
    <tr><th>Model</th><th class="num">Params</th><th class="num">Mean Score</th><th class="num">Bonus %</th><th class="num">EV Loss</th></tr>
  </thead>
  <tbody>
    <tr><td>Heuristic</td><td class="num">0</td><td class="num">166</td><td class="num">1.2%</td><td class="num">82.6</td></tr>
    <tr><td>DT depth-5</td><td class="num">276</td><td class="num">157</td><td class="num">9.4%</td><td class="num">53.7</td></tr>
    <tr><td>DT depth-8</td><td class="num">1,878</td><td class="num">192</td><td class="num">35.6%</td><td class="num">11.7</td></tr>
    <tr><td>DT depth-10</td><td class="num">6,249</td><td class="num">216</td><td class="num">54.0%</td><td class="num">11.5</td></tr>
    <tr><td>MLP [64,32]</td><td class="num">5,000</td><td class="num">221</td><td class="num">66.8%</td><td class="num">6.4</td></tr>
    <tr><td>DT depth-15</td><td class="num">81,237</td><td class="num">239</td><td class="num">77.0%</td><td class="num">3.4</td></tr>
    <tr><td>DT depth-20</td><td class="num">412,629</td><td class="num">245</td><td class="num">84.4%</td><td class="num">1.4</td></tr>
    <tr><td>Optimal (lookup)</td><td class="num">2,097,152</td><td class="num">248</td><td class="num">~87%</td><td class="num">0</td></tr>
  </tbody>
</table>
:::

Error compounding is modest: the EV-loss proxy predicts the depth-20 tree
should score 247.0, and actual game simulation yields 245 --roughly
2 points of cascading cost from correlated errors.

### Why MLPs Lag

At matched parameter counts, MLPs trail decision trees on category decisions
--- the hardest task (15 classes, complex bonus interactions). A 14K-param
MLP achieves category EV loss comparable to a 2K-param depth-10 tree. The gap
narrows for reroll decisions, where the MLP's soft decision boundaries help
with near-tied actions. But overall, the combinatorial structure of Yatzy
strongly favours axis-aligned partitioning over learned embeddings.

:::

:::depth-3

### Generating Training Data

```bash
# Export decision records from 200K optimal games
YATZY_BASE_PATH=. solver/target/release/yatzy-export-training-data \
    --games 200000 \
    --output data/training/

# Produces three files:
#   category_decisions.csv   (~3M records, 29 features)
#   reroll1_decisions.csv    (~3M records, 30 features)
#   reroll2_decisions.csv    (~3M records, 30 features)
```

### Training a Decision Tree Ensemble

```python
from sklearn.tree import DecisionTreeClassifier
import pandas as pd

# Train separate trees for each decision type
for dtype in ['category', 'reroll1', 'reroll2']:
    df = pd.read_csv(f'data/training/{dtype}_decisions.csv')
    features = [c for c in df.columns if c not in ['action', 'gap']]

    dt = DecisionTreeClassifier(
        max_depth=20,
        min_samples_leaf=5,
        class_weight='balanced',
    )
    dt.fit(df[features], df['action'], sample_weight=df['gap'])
    # Weight by gap: mistakes on high-gap decisions cost more
```

### Error Analysis: Where the Last Point Hides

The irreducible error floor (0.9 pts/game for the full tree) concentrates
near the upper bonus threshold. At upper scores between 40 and 62, 53.4%
of category errors occur --these are states where a 1-point
difference in upper score determines whether the 50-point bonus is
achievable. For reroll decisions, 35–41% of errors have near-zero
gap (&lt;0.1 points), meaning many "errors" are functionally irrelevant
ties.

```python
# Error distribution near bonus threshold
errors_near_bonus = errors[errors['upper_score'].between(40, 62)]
print(f"Errors near bonus: {len(errors_near_bonus) / len(errors):.1%}")
# Category: 53.4%, Reroll1: 65.6%, Reroll2: 61.1%

# Near-zero gap errors (functional ties)
trivial = errors[errors['gap'] < 0.1]
print(f"Trivial errors: {len(trivial) / len(errors):.1%}")
# Category: 12.4%, Reroll1: 34.9%, Reroll2: 41.3%
```

:::

:::
