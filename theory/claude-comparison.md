This is a seriously impressive body of work, Daniel. You've essentially replicated and extended the key results from the literature, but for Scandinavian Yatzy rather than American Yahtzee, and you've gone considerably deeper on the risk-sensitive dimension than anyone in the published literature. Let me walk through the comparison.
Your optimal solver matches the literature. Your EV-optimal mean of 248.4 for Scandinavian Yatzy aligns with the Larsson & Sjöberg (KTH, 2012) result of 248.63. The small difference likely comes from minor rule variants (there are several flavors of Scandinavian Yatzy). Your state-space architecture — bitmask of scored categories, upper-section progress, backward induction through roll/choice layers — is structurally identical to what Verhoeff and Glenn independently developed for American Yahtzee. You've clearly built your own solver from scratch in Rust, which is the right call given the performance requirements for the sweep work.
Your θ-sweep is genuinely novel. This is the most original contribution relative to the existing literature. Perkins (AAMAS 2019) explored tournament-optimal strategies for American Yahtzee, but his formulation is different from yours — he optimizes P(win) in an N-player blind tournament, which implicitly creates risk-seeking behavior but doesn't parameterize it continuously. Your approach of modifying the Bellman equation with V(s) = E[r] + θ·Var[r] (or whatever your exact formulation is — the document implies a mean-variance objective) and sweeping θ from -20 to +20 produces a complete characterization of the risk-return tradeoff that doesn't exist in the published literature for any Yahtzee variant.
A few specific comparisons worth noting:
The p95 peak and Perkins' tournament results tell the same story from different angles. Your finding that p95 peaks at θ≈0.07-0.09 with a +4 point gain at a cost of ~9 mean points is the Yatzy analog of Perkins' finding that tournament-optimal play sacrifices expected score for upper-tail probability. But you've quantified it far more precisely. Perkins showed that against 1 opponent, the tournament-optimal strategy scores 241.6±43 (vs 254.6±59.6 for EV-optimal) — a 13-point mean sacrifice for a 2.67% win-rate improvement. Your θ=0.08 gives mean=239.1 with a 4-point p95 gain — a similar magnitude of tradeoff. The key difference is that Perkins optimizes a discrete objective (P(win)) while you parameterize the risk preference continuously, which gives you the full frontier rather than isolated points.
Your two-branch mean-std analysis is new. Nobody in the literature has observed that risk-averse (θ<0) and risk-seeking (θ>0) trace distinct branches in mean-variance space. This is an elegant finding. The portfolio theory analogy is apt — but as you note, the classical efficient frontier is a single hyperbola because long/short positions are symmetric, while your branches differ because the mechanisms of variance compression vs. variance inflation are fundamentally different in a discrete combinatorial game. This would be publishable.
Your RL results confirm and extend the literature's findings, but with a crucial twist. The published RL work (Pape 2025, Häfner's Yahtzotron, Kang & Schroeder at Stanford) all report the same ~5% gap between RL and exact DP for American Yahtzee. Your Approach B (IQN with behavioral cloning) hit mean=205.3 — a 17% deficit, much worse than the literature's 5%. But this isn't an apples-to-apples comparison. The published RL agents are trained to play the game directly with millions of episodes of online experience, while your BC agent was trained on only 50K expert games and then failed to fine-tune. The 66%/81% accuracy numbers you report are actually consistent with Pape's observation that RL agents systematically fail on the upper-section bonus — your agent shows the same pattern.
The more interesting result is Approaches A and C, which the literature doesn't have an analog for. Nobody has tried meta-learning over precomputed strategy tables. Your finding that both REINFORCE and PPO collapse to fixed-θ-equivalent policies is a clean negative result. The signal-to-noise analysis is compelling: with per-turn θ choices affecting returns by <1 point against episode-level noise of σ≈38, you need ~1,400 episodes per learning step — technically achievable with 1-2M episodes, but the 15-action credit assignment problem kills it. This is a sharper version of the credit-assignment problem that Kang & Schroeder identified qualitatively.
Your per-category analysis goes far beyond anything published. The literature has per-category expected values (Verhoeff publishes these for American Yahtzee), but nobody has tracked how category statistics shift across a risk parameter. Your finding that Ones becomes the dump category (zero rate rising from 9% to 39% as θ increases) and the fill-order inversion (low-ceiling categories filled early, high-ceiling late) is a concrete operational description of what "risk-seeking" means in this game. The Yatzy hit-rate bump at θ≈0.05 is particularly interesting — mild risk-seeking actually helps Yatzy completion before higher θ starts sacrificing it.
Your "Why Humans Are Not Actually Good at Yatzy" section connects to a gap in the literature. There's no published work on human Yatzy skill estimation. Verhoeff has a "proficiency test" on his website, but it's a single-game score comparison, not a systematic analysis of decision errors. Your decomposition of the 20-30 point gap into four error categories (bonus mismanagement, reroll errors, category ordering, chase errors) with approximate point costs is original. The cascading suboptimality argument — that filling one category wrong propagates errors forward — is exactly the mechanism that makes Yahtzee hard for both humans and RL agents.
Where your work goes beyond the literature and where it doesn't:
Your θ-sweep, two-branch analysis, adaptive policy experiments, and per-category decomposition are all novel. The conditional Yatzy analysis (H0 vs H1 on the sacrifice hypothesis) is a clean piece of work that has no published analog.
What you haven't done that the literature has: exact rational arithmetic (Liese & Kelly computed the 139-digit exact fraction for American Yahtzee), multiplayer optimal play (Pawlewicz's score-distribution propagation for 2-player games), and the full CDF computation at every state (Glenn's 2006 technical report). These are mostly academic completionism rather than practically useful, though the full CDF work could feed into your adaptive θ framework.
One suggestion: Your revealed-preference / inverse-RL framework for estimating player θ from observed decisions is described but not yet implemented. This is where the practical value lies. With your solver infrastructure, you could build the first quantitative skill-assessment tool for Yatzy that distinguishes between "plays suboptimally because of computation errors" (low β) and "plays suboptimally because of risk preference" (non-zero θ). That decomposition doesn't exist for any dice game in the published literature.
