# Why reinforcement learning hits a wall in Yatzy

The persistent ~5% gap between RL agents and optimal play in Yahtzee/Yatzy is **primarily algorithmic, not fundamental** — but closing it requires solving three tightly coupled problems simultaneously: a discontinuous value function at the bonus threshold, a 13-turn credit assignment chain for the upper-section bonus, and neural network capacity too small to represent 8,192 distinct combinatorial regimes. The gap is concentrated overwhelmingly in one failure mode: RL agents achieve the upper-section bonus only **24.9% of the time versus ~68% under optimal play**, accounting for roughly 70–75% of the missing points. This isn't an abstract learning failure — agents make a specific, identifiable error, systematically placing four-of-a-kind rolls into the Four-of-a-Kind category for an immediate ~25 points rather than into the corresponding upper category for ~20 points, forfeiting a delayed 35–50 point bonus. The gap is not irreducible: DAgger with a DP oracle, potential-based reward shaping, and RUDDER-style credit assignment are all unexplored and theoretically well-matched to Yatzy's specific pathologies.

## The anatomy of 13 missing points

The best published RL result — Pape's (2025) A2C agent achieving a **241.78 median** versus the DP-optimal **254.59** — provides the clearest dissection of where the gap lives. The agent's upper-section bonus rate of 24.9% versus the optimal ~68% translates directly: missing the 35-point bonus an extra ~43% of the time costs **~15 expected points**, more than the entire observed gap. This means the agent actually *overperforms* slightly on other decisions, partially compensating for its bonus failures.

The error mechanism is precise. When an agent rolls four 5s, the immediate payoff from Four-of-a-Kind (~25 points, summing all dice) exceeds the payoff from Fives (20 points). A myopic agent takes the extra 5 points. But optimal play recognizes that placing 20 in Fives contributes toward the 63-point upper-section threshold, preserving access to a 35-point bonus. The net expected gain from the upper-section placement is positive — often substantially so — but this gain materializes only after several more turns of upper-section accumulation, creating a credit assignment chain spanning 5–10 turns.

The optimal strategy operates **right at the margin**: the expected upper-section sum under optimal play is ~62.77, just barely below the 63-point threshold. This means optimal play does *not* always prioritize the bonus — there are many states where abandoning the bonus is correct. The decision boundary between "pursue bonus" and "abandon bonus" is razor-thin and state-dependent, requiring precise tracking of remaining upper categories, accumulated upper sum, and remaining game turns. This is precisely the kind of sharp, context-dependent decision that smooth function approximators handle poorly.

Comparing across methods reveals a consistent pattern. Häfner's Yahtzotron achieves ~236–241 with A2C plus curriculum learning. Dutschke's MLP Q-learning reaches 241.6 under German Kniffel rules (~98.3% of that variant's optimal). Kang and Schroeder's MAXQ hierarchical approach manages only 129.6. Yuan's DQN catastrophically fails at 159 points. **Every method that exceeds 230 points uses actor-critic algorithms**; value-based methods (DQN, Q-learning without careful engineering) plateau far lower. This strongly suggests that policy gradient methods' ability to directly optimize expected returns gives them an edge over bootstrapped value estimation in high-variance environments.

## Why backgammon was solvable in 1992 but Yatzy remains unsolved

The contrast between TD-Gammon's 1992 triumph and Yatzy's persistent resistance illuminates a fundamental asymmetry in how dice randomness interacts with value function structure. In backgammon, dice are a **blessing** for learning. Each position's value is an expectation over 21 possible dice outcomes, and because similar board positions lead to similar outcome distributions, the averaging smooths the value function into a gently varying landscape. Tesauro's network — just **~5,000 parameters** with 40 hidden units — could approximate this smooth function well enough for superhuman play.

In Yatzy, dice randomness is a **curse**. The 252 distinct five-dice outcomes multiply the branching factor at every decision point, while the category-filling mechanism and bonus threshold create discontinuities that no amount of averaging can smooth. Two states differing only in whether Fives or Sixes is the sole remaining unfilled category have **value differences of 25+ points** for identical dice rolls. The bonus threshold at upper sum = 63 creates a step function worth 35–50 points. These are not small wrinkles — they are cliffs in the value landscape.

The structural comparison across stochastic games reveals a taxonomy of RL tractability driven by five axes. **Value function smoothness** is the most predictive: backgammon (smooth, RL succeeds), 2048 (moderate, RL succeeds with n-tuple networks), Yatzy (very rough, RL plateaus). **Credit assignment temporal structure** is second: poker has ~10–20 decisions per hand (short, tractable), backgammon has mostly local move consequences (tractable), Yatzy has a 13-turn cumulative bonus dependency (intractable for gradient methods). **Adversarial curriculum** is third: backgammon, Go, poker, and Mahjong all benefit from self-play that provides progressively harder challenges; Yatzy is solitaire with no opponent signal. **Reward density** matters enormously: 2048 provides immediate score feedback per merge, while Yatzy's most important reward (the bonus) is binary, delayed, and worth 14–20% of the typical score. **Stochastic branching** at decision points also matters, though less than the others: Yatzy's 252 outcomes per roll exceed backgammon's 21, but the more fundamental issue is that Yatzy's branching multiplies with its value function roughness rather than smoothing it.

These factors interact **multiplicatively**, not additively. Go handles moderate value function roughness through MCTS lookahead. Backgammon handles sparse rewards through smooth value functions. 2048 handles stochasticity through dense rewards and a trivial 4-action space. Yatzy faces rough value functions *plus* sparse delayed rewards *plus* high stochastic branching *plus* no adversarial curriculum — a combination that defeats every standard RL approach.

## Three interacting barriers create a perfect storm

**Barrier 1: The discontinuous value function exceeds neural network capacity.** The exact solver's lookup table stores ~536,448 between-turn state values in ~4 MB. The best RL network uses ~66K parameters — a 128× compression. But the value function has structure that resists compression. With 2^13 = 8,192 possible category-fill patterns (which categories remain open), the network must learn distinct value surfaces for each pattern. At 66K total parameters, only **~8 parameters are available per combinatorial regime**, far too few to accurately represent the value function within each regime, especially near the bonus threshold where values change sharply with upper-section sum.

Barron's approximation theorem predicts that single-hidden-layer networks achieve integrated squared error of O(C_f²/n) for n hidden nodes, where C_f is the Barron norm — essentially a measure of the function's smoothness in Fourier space. The bonus threshold's step function has heavy Fourier tails (large C_f), implying that many more nodes are needed. Yarotsky's results for piecewise smooth functions show that approximating functions with discontinuity surfaces in d dimensions requires O(ε^{-2(d-1)/β}) weights for L2 error ε, which grows rapidly with the number of dimensions along which discontinuities occur. In Yatzy, the discontinuity surface is the hyperplane where upper-section sum = 63 intersected with the combinatorial lattice of category fills — a high-dimensional, complex surface.

**Barrier 2: Credit assignment through 13 turns of stochastic noise.** The signal-to-noise ratio for individual decisions is devastating. A single per-turn θ choice affects expected returns by less than 1 point, while episode-level noise has σ ≈ 38 points (σ² ≈ 1,480). Detecting a 1-point improvement requires ~1,480 episodes at 1σ confidence — but this assumes perfect per-decision credit assignment. With 45 correlated decisions per episode, the effective sample complexity scales to **tens of thousands of episodes per detectable improvement**. The PAC-MDP bounds for tabular RL predict sample requirements of O(S²H³/ε²) ≈ 10^17 episodes for ε = 1 point accuracy, explaining why tabular convergence is impractical despite the modest state space.

The Singh-Yee bound formalizes how approximation error compounds: if the value function has max error ε everywhere, the performance loss of the greedy policy is bounded by 2Hε for horizon H. With H = 15 turns, the observed ~12-point gap implies per-turn approximation error of ~0.4 points — seemingly small, but concentrated at the bonus decision boundary where errors cost 35–50 points rather than fractions of a point.

**Barrier 3: No natural curriculum exists for solitaire optimization.** TD-Gammon's self-play creates an automatic curriculum: early random play provides easy opponents, and as the agent improves, opponents strengthen, constantly pushing toward better strategies. In AlphaGo, MCTS provides a similar bootstrapping mechanism. Yatzy has no opponent. The only training signal is the final score — a noisy, delayed scalar. Häfner's Yahtzotron demonstrates that curriculum learning is essential: without the three-stage curriculum (greedy → advantage → A2C), pure A2C converges to only ~100 points, trapped in greedy local optima. The advantage-lookup pre-training stage — which teaches the agent to evaluate "how much better than average is this roll for each category" — lifts performance to ~220 before A2C fine-tuning adds another ~16–20 points. This curriculum is hand-engineered, and its design required significant domain expertise.

## Unexplored approaches ranked by likelihood of success

Several well-established RL techniques have never been applied to Yatzy despite strong theoretical alignment with its specific pathologies.

**DAgger (Dataset Aggregation)** is the most promising unexplored approach. The core idea: train a neural network via behavioral cloning on DP-optimal demonstrations, roll out the learned policy, query the DP oracle at the states the network actually visits, and retrain on the aggregated dataset. This directly addresses distribution shift — the reason vanilla behavioral cloning achieves only 66%/81% action accuracy despite 1.5M training examples. Ross and Bagnell proved that DAgger reduces the error compounding rate from O(T²ε) to O(Tε). For Yatzy, the DP oracle is exact and fast (~30 seconds to compute the full strategy table), making oracle queries essentially free. Implementation requires only iterating between policy rollouts and oracle labeling — no RL training loop, no reward shaping, no hyperparameter sensitivity. Predicted gap closure: **3–4 percentage points**.

**Potential-based reward shaping** directly addresses the bonus credit assignment problem. Ng et al. (1999) proved that adding F(s,a,s') = γΦ(s') - Φ(s) preserves the optimal policy. A natural potential function for Yatzy: Φ(s) = (estimated probability of achieving bonus given current upper-section state) × (bonus value). This provides immediate reward signal whenever a decision moves the agent toward or away from the bonus threshold, converting a 13-turn credit assignment problem into per-turn feedback. Even a crude approximation of bonus probability — based solely on upper-section sum and number of remaining upper categories — would provide substantial gradient signal. Recent work on Bootstrapped Reward Shaping (BSRS, 2025) shows that using the agent's own learned value estimate as the potential can work with just one line of code change. Predicted gap closure: **1–3 percentage points**.

**RUDDER (Return Decomposition for Delayed Rewards)** trains an LSTM to predict episode returns from state-action sequences, then uses contribution analysis to decompose the return into per-step attributions. This is precisely the credit assignment mechanism Yatzy needs: RUDDER would identify that filling Ones with 2 points (rather than sacrificing the category) was the decision most correlated with bonus achievement. Yatzy's 39-step episodes are well within LSTM capacity. The technique achieved exponential speedups over TD and Monte Carlo on synthetic delayed-reward tasks. Predicted gap closure: **2–3 percentage points**.

**Stochastic MuZero** could provide the highest ceiling at the cost of significant computational expense. DeepMind's Stochastic MuZero (2022) handles stochastic environments through afterstates and chance codes, matching AlphaZero with perfect simulator in 2048 and backgammon. For Yatzy, since the dynamics are fully known, a simpler AlphaZero-style MCTS with exact chance nodes could be used. The challenge is the 252-way branching at each dice roll, which makes deep tree search expensive — but sparse sampling and progressive widening can mitigate this. Gabillon et al. (JMLR 2017) applied UCT to Yahtzee with 120 seconds per move, finding that the stochastic branching limited search depth to 2–3 plies. A neural network value function would enable much deeper effective search. Predicted gap closure: **2–4 percentage points at inference time**.

These techniques are **combinable**: DAgger + reward shaping + distributional RL is a natural stack that addresses distribution shift, delayed rewards, and bonus bimodality simultaneously.

## Is the gap fundamental or can unlimited compute close it?

There is no formal proof that polynomial-size neural networks cannot approximate the Yatzy value function within ε = 1 point. The gap appears to be **primarily a sample efficiency and optimization issue** compounded by insufficient network capacity — not an information-theoretic impossibility. Three lines of evidence support this:

First, **the lookup table is small**. At ~4 MB for between-turn state values, the Yatzy value function has modest Kolmogorov complexity. A sufficiently large neural network (say, 500K–1M parameters, still far smaller than modern language models) should have the capacity to represent it. The 66K-parameter networks used in published work are likely too small by a factor of **5–10×**, given the 8,192 combinatorial regimes each requiring distinct value surfaces.

Second, **tabular RL would converge** given enough samples. The between-turn state space of 536K states is tractable for tabular methods in principle. No published work has tested pure tabular Q-learning on solitaire Yahtzee to convergence — this is a surprising gap in the literature. If tabular RL converges to optimal, the entire gap is attributable to function approximation and sample efficiency, not to any fundamental RL limitation.

Third, **the DP solver proves that the problem is computationally easy**. The optimal strategy can be computed by backward induction in 30 seconds. The challenge is not computational hardness but rather that RL must *discover* what DP computes *directly* — learning from noisy episodic returns what backward induction extracts from the Bellman equation in closed form. This is an efficiency gap, not a computability gap.

However, there is a sense in which the gap may be **practically irreducible for standard RL architectures at standard compute budgets**. The PAC-MDP sample complexity bounds, even under favorable assumptions, predict sample requirements in the billions for ε = 1 point accuracy with the full state space. The combinatorial explosion of category-fill patterns creates a learning problem where each of 8,192 regimes must be learned quasi-independently, with limited transfer between regimes. Architectures that explicitly encode the bonus threshold — say, with a dedicated input feature for distance-to-bonus and a gating mechanism — could dramatically reduce the effective problem difficulty, but this amounts to building domain knowledge into the architecture rather than learning it.

## A practical blueprint for the next attempt

Anyone building a competitive RL Yatzy agent should follow this architecture:

The **state representation** must explicitly encode upper-section progress. Include: a binary vector of filled categories (15 bits for Yatzy, 13 for Yahtzee), the upper-section running sum, a binary "bonus achieved" flag, a binary "bonus still achievable" flag, the distance to the bonus threshold (upper_sum − 63), dice values as face-count vectors (6 integers), and rerolls remaining. The distance-to-threshold feature transforms the discontinuous bonus problem into a smoother one.

The **network architecture** should use **250K–500K parameters** — roughly 5–10× larger than published attempts. Use separate heads for reroll masks and category selection with a shared trunk, following Pape's multi-headed design. Include a dedicated "bonus branch" that processes upper-section features through its own layers before merging with the main trunk. The reroll head should exploit **dice exchangeability** — dice are unordered, so the network should process dice-face counts rather than individual die values.

The **training curriculum** should follow a four-stage process. Stage 1: Generate 10M state-action pairs from the DP oracle and train via behavioral cloning to ~90% action accuracy. Stage 2: Apply **DAgger** for 50–100 iterations, rolling out the learned policy, querying the oracle at visited states, and retraining on the aggregated dataset. Stage 3: Add **potential-based reward shaping** using Φ(s) = P(bonus | upper_sum, remaining_upper_categories) × bonus_value, and fine-tune with A2C using TD-λ scheduling (λ: 0.2 → 0.8). Stage 4: Apply **RUDDER** to decompose episode returns and provide per-decision credit signal for the remaining gap.

The **minimum compute budget** is modest: Häfner achieved ~236 points in 2 hours on a single CPU. With a larger network and DAgger iterations, expect **8–24 hours on a single GPU** for the full pipeline. The DP oracle computation takes 30 seconds and can be precomputed.

## Conclusion

Yatzy occupies a uniquely adversarial position in the landscape of RL-tractable games — not because any single feature makes it hard, but because its pathologies are **mutually reinforcing**. The value function's bonus-threshold discontinuity demands high network capacity. The long-horizon credit assignment for the bonus demands enormous sample sizes. The high stochastic variance drowns gradient signals. And the solitaire setting provides no adversarial curriculum to bootstrap learning. Each factor alone is manageable — Go handles value function roughness through MCTS, backgammon handles stochastic variance through smooth value functions, 2048 handles solitaire settings through dense rewards. Yatzy combines the worst of all worlds.

The most important novel insight is that the gap is **not spread uniformly across decision types** — it is overwhelmingly concentrated in upper-section bonus management, a single strategic dimension that creates the game's hardest credit assignment problem. This suggests that targeted interventions (reward shaping for bonus progress, explicit bonus-aware architecture, DAgger with an oracle) are more promising than general-purpose RL improvements. The 5% gap is not a wall — it is a door with a known lock, waiting for the right key.