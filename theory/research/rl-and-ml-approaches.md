# RL and ML Approaches to Yatzy

This work replicates and extends the key results from the literature for Scandinavian Yatzy rather than American Yahtzee, going considerably deeper on the risk-sensitive dimension than anyone in the published literature. The EV-optimal mean of 248.4 aligns with Larsson & Sjöberg (KTH, 2012) at 248.63. The θ-sweep is genuinely novel — Perkins (AAMAS 2019) explored tournament-optimal strategies for American Yahtzee, but with a different formulation (optimizing P(win) in an N-player blind tournament). The two-branch mean-variance analysis, adaptive policy experiments, per-category decomposition, and conditional Yatzy analysis all have no published analog. The revealed-preference / inverse-RL framework for estimating player θ from observed decisions would be the first quantitative skill-assessment tool for any dice game that distinguishes between computation errors (low β) and risk preference (non-zero θ).

---

# Why Reinforcement Learning Hits a Wall in Yatzy

The persistent ~5% gap between RL agents and optimal play in Yahtzee/Yatzy is **primarily algorithmic, not fundamental** — but closing it requires solving three tightly coupled problems simultaneously: a discontinuous value function at the bonus threshold, a 13-turn credit assignment chain for the upper-section bonus, and neural network capacity too small to represent 8,192 distinct combinatorial regimes. The gap is concentrated overwhelmingly in one failure mode: RL agents achieve the upper-section bonus only **24.9% of the time versus ~68% under optimal play**, accounting for roughly 70–75% of the missing points. This isn't an abstract learning failure — agents make a specific, identifiable error, systematically placing four-of-a-kind rolls into the Four-of-a-Kind category for an immediate ~25 points rather than into the corresponding upper category for ~20 points, forfeiting a delayed 35–50 point bonus. The gap is not irreducible: DAgger with a DP oracle, potential-based reward shaping, and RUDDER-style credit assignment are all unexplored and theoretically well-matched to Yatzy's specific pathologies.

## The anatomy of 13 missing points

The best published RL result — Pape's (2025) A2C agent achieving a **241.78 median** versus the DP-optimal **254.59** — provides the clearest dissection of where the gap lives. The agent's upper-section bonus rate of 24.9% versus the optimal ~68% translates directly: missing the 35-point bonus an extra ~43% of the time costs **~15 expected points**, more than the entire observed gap. This means the agent actually *overperforms* slightly on other decisions, partially compensating for its bonus failures.

The error mechanism is precise. When an agent rolls four 5s, the immediate payoff from Four-of-a-Kind (~25 points, summing all dice) exceeds the payoff from Fives (20 points). A myopic agent takes the extra 5 points. But optimal play recognizes that placing 20 in Fives contributes toward the 63-point upper-section threshold, preserving access to a 35-point bonus. The net expected gain from the upper-section placement is positive — often substantially so — but this gain materializes only after several more turns of upper-section accumulation, creating a credit assignment chain spanning 5–10 turns.

The optimal strategy operates **right at the margin**: the expected upper-section sum under optimal play is ~62.77, just barely below the 63-point threshold. This means optimal play does *not* always prioritize the bonus — there are many states where abandoning the bonus is correct. The decision boundary between "pursue bonus" and "abandon bonus" is razor-thin and state-dependent, requiring precise tracking of remaining upper categories, accumulated upper sum, and remaining game turns. This is precisely the kind of sharp, context-dependent decision that smooth function approximators handle poorly.

Comparing across methods reveals a consistent pattern. Häfner's Yahtzotron achieves ~236–241 with A2C plus curriculum learning. Dutschke's MLP Q-learning reaches 241.6 under German Kniffel rules (~98.3% of that variant's optimal). Kang and Schroeder's MAXQ hierarchical approach manages only 129.6. Yuan's DQN catastrophically fails at 159 points. **Every method that exceeds 230 points uses actor-critic algorithms**; value-based methods (DQN, Q-learning without careful engineering) plateau far lower. This strongly suggests that policy gradient methods' ability to directly optimize expected returns gives them an edge over bootstrapped value estimation in high-variance environments.

## Why backgammon was solvable in 1992 but Yatzy remains unsolved

The contrast between TD-Gammon's 1992 triumph and Yatzy's persistent resistance illuminates a fundamental asymmetry in how dice randomness interacts with value function structure. In backgammon, dice are a **blessing** for learning. Each position's value is an expectation over 21 possible dice outcomes, and because similar board positions lead to similar outcome distributions, the averaging smooths the value function into a gently varying landscape. Tesauro's network — just **~5,000 parameters** with 40 hidden units — could approximate this smooth function well enough for superhuman play.

In Yatzy, dice randomness is a **curse**. The 252 distinct five-dice outcomes multiply the branching factor at every decision point, while the category-filling mechanism and bonus threshold create discontinuities that no amount of averaging can smooth. Two states differing only in whether Fives or Sixes is the sole remaining unfilled category have **value differences of 25+ points** for identical dice rolls. The bonus threshold at upper sum = 63 creates a step function worth 35–50 points. These are not small wrinkles — they are cliffs in the value landscape.

The structural comparison across stochastic games reveals a taxonomy of RL tractability driven by five axes. **Value function smoothness** is the most predictive: backgammon (smooth, RL succeeds), 2048 (moderate, RL succeeds with n-tuple networks), Yatzy (very rough, RL plateaus). **Credit assignment temporal structure** is second: poker has ~10–20 decisions per hand (short, tractable), backgammon has mostly local move consequences (tractable), Yatzy has a 13-turn cumulative bonus dependency (intractable for gradient methods). **Adversarial curriculum** is third: backgammon, Go, poker, and Mahjong all benefit from self-play that provides progressively harder challenges; Yatzy is solitaire with no opponent signal. **Reward density** matters enormously: 2048 provides immediate score feedback per merge, while Yatzy's most important reward (the bonus) is binary, delayed, and worth 14–20% of the typical score. **Stochastic branching** at decision points also matters, though less than the others: Yatzy's 252 outcomes per roll exceed backgammon's 21, but the more fundamental issue is that Yatzy's branching multiplies with its value function roughness rather than smoothing it.

These factors interact **multiplicatively**, not additively. Go handles moderate value function roughness through MCTS lookahead. Backgammon handles sparse rewards through smooth value functions. 2048 handles solitaire settings through dense rewards and a trivial 4-action space. Yatzy combines the worst of all worlds.

## Three interacting barriers create a perfect storm

**Barrier 1: The discontinuous value function exceeds neural network capacity.** The exact solver's lookup table stores ~536,448 between-turn state values in ~4 MB. The best RL network uses ~66K parameters — a 128× compression. But the value function has structure that resists compression. With 2^13 = 8,192 possible category-fill patterns (which categories remain open), the network must learn distinct value surfaces for each pattern. At 66K total parameters, only **~8 parameters are available per combinatorial regime**, far too few to accurately represent the value function within each regime, especially near the bonus threshold where values change sharply with upper-section sum.

**Barrier 2: Credit assignment through 13 turns of stochastic noise.** The signal-to-noise ratio for individual decisions is devastating. A single per-turn θ choice affects expected returns by less than 1 point, while episode-level noise has σ ≈ 38 points (σ² ≈ 1,480). Detecting a 1-point improvement requires ~1,480 episodes at 1σ confidence — but this assumes perfect per-decision credit assignment.

**Barrier 3: No natural curriculum exists for solitaire optimization.** TD-Gammon's self-play creates an automatic curriculum: early random play provides easy opponents, and as the agent improves, opponents strengthen, constantly pushing toward better strategies. Yatzy has no opponent. The only training signal is the final score — a noisy, delayed scalar.

## Unexplored approaches ranked by likelihood of success

**DAgger (Dataset Aggregation)** is the most promising unexplored approach. The DP oracle is exact and fast (~30 seconds to compute the full strategy table), making oracle queries essentially free. Predicted gap closure: **3–4 percentage points**.

**Potential-based reward shaping** directly addresses the bonus credit assignment problem. Ng et al. (1999) proved that adding F(s,a,s') = γΦ(s') - Φ(s) preserves the optimal policy. A natural potential function: Φ(s) = P(bonus | upper_sum, remaining_upper_categories) × bonus_value. Predicted gap closure: **1–3 percentage points**.

**RUDDER (Return Decomposition for Delayed Rewards)** trains an LSTM to predict episode returns from state-action sequences, then uses contribution analysis to decompose the return into per-step attributions. Predicted gap closure: **2–3 percentage points**.

**Stochastic MuZero** could provide the highest ceiling at the cost of significant computational expense. Predicted gap closure: **2–4 percentage points at inference time**.

These techniques are **combinable**: DAgger + reward shaping + distributional RL is a natural stack.

## Is the gap fundamental or can unlimited compute close it?

There is no formal proof that polynomial-size neural networks cannot approximate the Yatzy value function within ε = 1 point. The gap appears to be **primarily a sample efficiency and optimization issue** compounded by insufficient network capacity — not an information-theoretic impossibility. The lookup table is small (~4 MB), tabular RL would converge given enough samples, and the DP solver proves that the problem is computationally easy. However, the PAC-MDP sample complexity bounds predict sample requirements in the billions for ε = 1 point accuracy, suggesting the gap may be **practically irreducible for standard RL architectures at standard compute budgets**.
