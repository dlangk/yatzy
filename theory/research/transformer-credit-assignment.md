# Transformers can likely close the Yatzy bonus gap, but not through the mechanism you'd expect

**The short answer is yes — but the mechanism is pattern matching on optimal trajectories, not learned credit assignment.** A Decision Transformer trained on DP-optimal play data, modified with ESPER-style stochasticity corrections, represents the most viable path to closing the 43-percentage-point bonus rate gap (24.9% vs. 68.1%) that accounts for 70–75% of the performance deficit between RL agents and optimal play. The critical insight, however, comes from Ni et al. (2023, NeurIPS Oral): **transformers excel at memory but do not fundamentally improve temporal credit assignment over LSTMs**. For Yatzy's 15-turn horizon, this distinction matters less than the choice of training paradigm — and the unique availability of unlimited DP-optimal trajectories makes offline sequence modeling approaches unusually well-suited.

The Yatzy bonus problem has a precise structure that makes it a near-ideal testbed. When an RL agent holds four 5s and places them in Four-of-a-Kind (~25 points) rather than Fives (20 points), it sacrifices a 35-point delayed bonus requiring 5–10 more turns of coordinated upper-section accumulation to reach the 63-point threshold. Every major RL approach tried — A2C, PPO, REINFORCE, DQN — plateaus at ~241 points versus the optimal 254.59, with the bonus rate gap as the dominant failure mode. No transformer-based Yahtzee agent exists in the published literature, making this a clear research opportunity.

## The stochasticity problem is the real obstacle, not credit assignment length

The most critical paper for this application is not about credit assignment at all — it is Paster et al.'s "You Can't Count on Luck" (NeurIPS 2022), which proves that standard Decision Transformers fail in stochastic environments. When DT conditions on high return-to-go, it reproduces actions from high-return trajectories indiscriminately, including actions that achieved high returns through lucky dice rolls rather than superior strategy. Yahtzee's dice-rolling stochasticity falls squarely into this failure mode.

**ESPER** (Environment-Stochasticity-Independent Representations) fixes this through adversarial trajectory clustering. It learns representations that capture strategic differences between trajectories while being provably independent of environmental stochasticity. A cluster model groups trajectories by behavior patterns; an adversarial transition predictor ensures the clustering cannot predict dice outcomes. Cluster-average returns then replace raw returns as conditioning targets. ESPER was validated on 2048 (stochastic tile placement) and Connect Four (stochastic opponent) — structurally analogous to Yahtzee's dice randomness. The **Dichotomy of Control** framework (Yang et al., ICLR 2023) provides a complementary approach, using mutual information constraints to separate controllable actions from uncontrollable transitions.

For Yatzy specifically, ESPER would learn to group games by category-selection strategy quality rather than by final score. A game scoring 200 due to bad dice but excellent placement decisions would cluster with other strategically sound games, not with genuinely poor play that happened to also score 200. This directly addresses the confound that defeats standard DT.

## Three mechanisms through which transformers might solve the bonus problem

The hypothesis that transformers solve credit assignment "through self-attention over the sequence of game decisions" decomposes into three distinct mechanisms, each with different empirical support.

**Pattern matching on trajectory shapes** is the most likely mechanism and the one best supported by evidence. When conditioned on RTG ≥ 280 (implying the bonus was achieved), a DT trained on DP-optimal trajectories would reproduce the action patterns observed in high-scoring games. Since optimal play achieves the bonus in ~68% of games, high-RTG trajectories in the training data overwhelmingly feature upper-section prioritization. The model learns the statistical association: high RTG → place dice in upper categories early, even at immediate cost. This is not credit assignment in the causal sense — it is filtered behavior cloning with return conditioning. Emmons et al. (ICLR 2022) showed that a simple 2-layer MLP with reward conditioning matches DT performance on D4RL benchmarks, suggesting the transformer architecture is not essential for this mechanism. However, for Yatzy's 135-token trajectory context, the transformer's ability to attend across all 15 turns simultaneously may provide genuine advantage over Markovian approaches.

**Implicit planning via trajectory modeling** is the mechanism offered by the Trajectory Transformer (Janner et al., NeurIPS 2021), which models the full joint distribution over (state, action, reward) tuples and uses beam search over trajectory completions. Beam search can explicitly evaluate whether a current placement decision leads to bonus achievement within the planning horizon. Janner et al. noted that combining the Trajectory Transformer with dynamic programming yields "a state-of-the-art planner for sparse-reward, long-horizon tasks" — precisely the Yatzy bonus structure. The computational cost is significant (beam width 32–256 × horizon × state dimensions per action), but Yatzy's compact state space (~50–100 features, 13 categories) makes this tractable.

**Learned credit attribution via attention** is the most theoretically appealing but least empirically supported mechanism. The hypothesis that a "bonus tracking head" would attend from the bonus-receiving turn back to earlier upper-section placements is mechanistically plausible — transformers can learn running sums, threshold functions, and conditional logic. Nanda et al. (2023) showed Othello-GPT develops linear representations of board state extractable via probes, and the balanced-bracket classification literature demonstrates transformers learning to track cumulative quantities against thresholds. However, Ni et al. (2023) provides the sobering counterpoint: in online model-free RL settings, transformers do not improve credit assignment over LSTMs. Both architectures fail at credit assignment beyond ~250 steps, though Yatzy's 15-turn horizon is well within the regime where both succeed.

## Ranked assessment of transformer approaches

The following ranking reflects both theoretical fit and practical feasibility given existing Rust DP solver infrastructure.

**Tier 1: ESPER-DT on DP-optimal trajectories.** This is the highest-priority approach. The DP solver can generate unlimited optimal trajectories (each game is ~2,000 tokens; millions of games are feasible on a single machine). ESPER's adversarial clustering separates strategy quality from dice luck. The stitching problem — DT's most fundamental limitation, proven by Brandfonbrener et al. (NeurIPS 2022) — is entirely circumvented because the training data already contains optimal complete trajectories. No stitching needed. RTG conditioning at test time naturally produces bonus-pursuing behavior: conditioning on RTG ≥ 280 reproduces the action patterns of games that achieved the bonus. The 135-token full-game context fits comfortably within standard transformer architectures. A small model (2–4 layers, 128–256 dimensions) trained on 100K–1M optimal games would likely suffice, trainable in hours on a single GPU. Bhargava et al. (ICLR 2024) showed DT continues improving with more data and can learn from trajectories of varying quality, but the availability of purely optimal data is an unusual advantage that eliminates the data quality problem entirely.

**Tier 2: Transformer-RUDDER / ARES reward shaping.** Rather than replacing the RL agent's architecture, this approach transforms the reward signal. Liu et al. (2019) demonstrated transformer-based return decomposition — training a transformer to predict episode returns from state-action sequences, then using prediction differences as per-step reward attributions. Holmes & Chi's ARES (2025) extends this with attention-based reward shaping that works offline, is robust to extreme reward delays, and is compatible with any RL algorithm. For Yatzy, a transformer trained to predict final game scores would learn that upper-section placements near the 63-point threshold cause the largest prediction jumps — directly identifying the decisions that matter most for the bonus. These decomposed rewards could then be fed to any standard RL algorithm (A2C, PPO) as dense shaped rewards, potentially solving the bonus problem without changing the agent architecture at all. This approach is complementary to Tier 1 and could be used as a diagnostic tool even if not the primary training method.

**Tier 3: Potential-based reward shaping with DP-derived values.** This is not transformer-based but is the most theoretically grounded solution and the most practical given existing infrastructure. The DP solver already computes V*(s) for all states. Using Φ(s) = V*(s) as the potential function for PBRS (Ng et al., 1999) provides dense, intermediate rewards that are provably policy-invariant — they cannot introduce spurious optima. The shaped reward F(s, a, s') = γΦ(s') − Φ(s) would provide immediate positive signal when an upper-section placement advances toward the 63-point threshold and negative signal when it doesn't. Dynamic PBRS (Devlin & Kudenko, 2012) further allows time-varying potentials, capturing the fact that progress toward 63 is more valuable with more categories remaining. This approach requires minimal implementation effort beyond the existing DP solver.

**Tier 4: Trajectory Transformer with DP-guided beam search.** Strong planning capability for multi-turn bonus coordination, but computationally expensive at inference time. Best suited as a diagnostic tool to verify whether planning-based approaches can close the bonus gap, rather than as a deployment architecture.

**Tier 5: GTrXL for online RL.** Memory-augmented transformers could implicitly track upper-section running totals, but Ni et al.'s finding that transformers don't improve credit assignment over LSTMs dampens expectations. For Yatzy's short sequences, memory is not the bottleneck.

**Tier 6: Algorithm Distillation.** Generating learning histories from progressively improving Yatzy agents is conceptually sound, but the training pipeline is complex and the approach is designed for meta-learning across tasks, not single-game mastery. Noise distillation (Zisman et al., 2023) offers a simpler variant.

## What attention patterns would actually reveal

If a DT were trained on Yatzy and subjected to mechanistic interpretability analysis, the expected findings would likely disappoint the "attention as credit assignment" hypothesis while providing genuinely useful insights.

**Linear probing** would almost certainly succeed at extracting the running upper-section sum from intermediate hidden states. The Othello-GPT precedent (Nanda et al., 2023) — where linear probes extracted full board state from a game-playing transformer — and the broader linear representation hypothesis (Park et al., 2023) strongly predict that Yatzy game state features would be linearly encoded. This includes the upper-section running total, the number of remaining categories, and likely a "distance to bonus threshold" quantity. Confirming this would validate that the model internally tracks the relevant information, even if the computation is distributed across layers rather than concentrated in a single "bonus tracking head."

**Attention pattern analysis** would likely reveal that the model attends primarily to recent state tokens and RTG tokens rather than forming long-range credit assignment chains back to specific earlier decisions. Joseph Bloom's interpretability work (2022) on a 1-layer MiniGrid DT found specialized heads for obstacle avoidance and goal-seeking, with RTG embeddings projecting into action-relevant directions. For Yatzy, we'd expect to find heads that attend to the state representation (which categories are filled, current scores) rather than retrospectively attributing credit. The credit assignment would be implicit in the RTG conditioning, not explicit in attention flows.

**Causal intervention** via head ablation would identify which components are necessary for bonus-pursuing behavior. The methodology is mature — Michel et al. (2019) showed systematic head ablation reveals specialization, and Causal Head Gating (2025) provides graded scores for facilitation vs. interference. For Yatzy, ablating the heads that attend to upper-section state features should collapse bonus rates. This would confirm that the model uses state information about upper-section progress but would not demonstrate backward credit attribution from the bonus reward to earlier decisions.

The distinction matters: the transformer doesn't "attribute the bonus reward back to specific earlier decisions." It learns that in trajectories with high returns, players make upper-section placements early. The attention mechanism enables the model to condition current decisions on the full game history, but the credit assignment is in the data distribution, not in the model's computation.

## How the bonus threshold discontinuity interacts with each approach

The 35-point step function at exactly 63 upper-section points creates a **bimodal return distribution** — games with the bonus and games without form distinct clusters separated by ~35 points. This discontinuity interacts differently with each approach.

For DT with RTG conditioning, the bimodality is actually helpful. High RTG values (≥ ~270) correspond almost exclusively to bonus-achieving trajectories, while moderate RTG values (≥ ~235) include a mix. The model naturally segments its behavior by RTG range: conditioned on high RTG, it reproduces bonus-pursuing strategies; conditioned on moderate RTG, it produces strategies that forgo the bonus in favor of immediate scoring. The threshold at 63 creates a sharp decision boundary that transformers can represent — evidence from arithmetic transformers shows they handle digit-level carry operations and step functions via MLP layers. The challenge is not representational capacity but data coverage near the boundary: the model needs sufficient training examples of games where the upper-section total was 60–66 to learn the critical behavior at the threshold margin.

For Trajectory Transformer beam search, the discontinuity creates an evaluation challenge. Beam scoring must account for the possibility that a trajectory currently at 58 upper-section points could reach 63 with future placements. This requires either a long planning horizon (covering the remaining upper-section decisions) or integration with a value function that captures the bonus potential — the Q-guided beam search variant that Janner et al. described as "combining sequence modeling with conventional RL."

For reward decomposition approaches (RUDDER/ARES), the discontinuity is precisely what makes them effective. The largest prediction jumps in a return-predicting transformer will occur at the decision that pushes the upper-section total past 63 — this is where the model's estimate of the final return increases by ~35 points. The decomposed reward at that step would be correspondingly large, and the decomposed rewards at earlier upper-section placements that contributed toward the threshold would also be elevated. This provides exactly the reward shaping signal that current RL agents lack.

## Practical implementation path with existing infrastructure

Given a Rust DP solver that computes optimal play, the most efficient implementation path uses a three-phase approach.

**Phase 1 (1–2 weeks): Baseline data generation and PBRS.** Generate 1M optimal game trajectories from the DP solver with random dice rolls, recording full (state, action, reward) sequences. Implement PBRS using V*(s) from the DP solver as the potential function. Retrain A2C/PPO with shaped rewards. This alone may close most of the bonus gap — it is the lowest-risk intervention because PBRS is provably policy-invariant and requires no new architecture. Expected bonus rate improvement: from **25% to potentially 50–60%**, based on the fact that PBRS directly addresses the reward signal sparsity that prevents learning the bonus strategy.

**Phase 2 (2–4 weeks): ESPER-DT training.** Implement a small Decision Transformer (GPT-2-style, 2–4 layers, 128–256 hidden dim) in Python/PyTorch. Tokenize Yatzy trajectories as (RTG, state, action) triplets — ~135 tokens per game, well within context limits. Apply ESPER's adversarial clustering to separate strategic quality from dice luck. Train on the 1M optimal trajectories. Evaluate bonus rate when conditioning on high RTG (≥ 280). Expected compute: **hours on a single GPU** for a model of this size. The key validation metric is whether RTG conditioning at test time produces the correct qualitative behavior: upper-section prioritization when conditioned on high returns.

**Phase 3 (2–4 weeks): Interpretability and diagnostics.** Train linear probes on intermediate hidden states to extract running upper-section sums. Perform head ablation studies to identify which attention heads are necessary for bonus-pursuing behavior. Train a transformer-RUDDER model to decompose game scores into per-turn contributions and verify that upper-section placements near the 63-point threshold receive the highest attributions. Use these diagnostics to understand whether the model has truly learned the bonus strategy or is merely correlating with surface-level trajectory features.

## The deeper question: does credit assignment even need transformers?

The most provocative finding across this research is that the Yatzy bonus problem may not require transformers at all. The credit assignment chain is 5–10 turns — well within LSTM capacity. The real bottleneck in current RL approaches is not architectural but algorithmic: standard policy gradient methods with episodic rewards provide insufficient gradient signal to learn the bonus strategy because the bonus is rare under random exploration (the bonus only materializes in ~25% of trained-agent games, and far less during early training).

**PBRS with DP-derived potentials** solves this problem without any sequence modeling. **Return decomposition** (whether LSTM-based RUDDER or transformer-based) solves it by converting the sparse bonus into dense per-step rewards. **DT's RTG conditioning** solves it by sidestepping credit assignment entirely — the model doesn't need to attribute the bonus to earlier decisions because it directly imitates the action patterns of bonus-achieving trajectories.

The transformer's genuine contribution, if any, would be in the **interpretability** dimension. Attention patterns over game trajectories provide a natural visualization of which past decisions the model considers when making current choices — a capability that LSTMs and SSMs lack. For understanding *why* the bonus strategy is difficult and *how* optimal play coordinates upper-section decisions, transformer interpretability tools offer unique scientific value even if the performance gains could be achieved with simpler architectures.

## Conclusion

The ranked assessment places **ESPER-DT on DP-optimal data** as the most promising transformer approach, followed by **transformer-based return decomposition** and **DP-derived potential-based reward shaping**. The mechanism by which transformers would close the bonus gap is primarily pattern matching on optimal trajectory shapes via RTG conditioning — not learned backward credit attribution through attention. The stochasticity of dice rolls, not the credit assignment horizon, is the binding constraint that must be addressed (via ESPER or DoC). The unique availability of unlimited DP-optimal training data eliminates DT's stitching limitation and makes this a best-case scenario for offline sequence modeling. For practical purposes, PBRS with the existing DP solver should be tried first as a low-risk, high-impact intervention, with transformer approaches reserved for cases where PBRS alone proves insufficient or where interpretability of the learned strategy is a primary goal. The expected outcome is that the combination of stochasticity-aware offline learning and DP-derived reward shaping can push the bonus rate from ~25% toward 50–60%, closing the majority of the performance gap — though reaching the optimal 68% may require additional innovations in how the agent handles the stochastic interaction between dice outcomes and threshold-crossing decisions.